{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential Privacy_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagirathtallapragada/Secure-AI-project-phase2/blob/main/Differential_Privacy_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lRryRN_sQCr"
      },
      "source": [
        "In this notebook, I train differentially private models on the CIFAR-100 dataset. \n",
        "1. Use TensorFlow Privacy which provide DP implementations of standard optimizers, but you only need to pick one. \n",
        "2. The goal is to train a well performing model while keeping a small value for epsilon during training. \n",
        "3. Use a fixed delta of 4 × 10−5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aqKJ4z-tOsU",
        "outputId": "71c72703-ec17-4de5-9823-47c7dc8c57ca"
      },
      "source": [
        "# mounting the drive to store the results\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #, force_remount = True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsOQWJJduM9q",
        "outputId": "84199d53-0b24-4cb9-a027-a701b4b10bb0"
      },
      "source": [
        "!pip install keras==2.2.3 tensorflow_privacy==0.2.2 --quiet # use these versions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 312 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 82 kB 705 kB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.2.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIWPtp5O1BA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b812df8-b0fe-4170-aa02-75e8f7f4ee03"
      },
      "source": [
        "# !pip install tensorflow-privacy --quiet # skip this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 251 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 46.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2DcXWzu4Qv4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYT8WsDsqrkw"
      },
      "source": [
        "# To solve the compatibility issues between tensorflow privacy and keras in invoking differentially private optimizers\n",
        "\n",
        "from absl import logging\n",
        "import collections\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
        "\n",
        "def make_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
        "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
        "  child_code = cls.compute_gradients.__code__\n",
        "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
        "  if child_code is not parent_code:\n",
        "    logging.warning(\n",
        "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
        "        'method compute_gradients(). Check to ensure that '\n",
        "        'make_optimizer_class() does not interfere with overridden version.',\n",
        "        cls.__name__)\n",
        "\n",
        "  class DPOptimizerClass(cls):\n",
        "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
        "\n",
        "    _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dp_sum_query,\n",
        "        num_microbatches=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
        "        **kwargs):\n",
        "      \"\"\"Initialize the DPOptimizerClass.\n",
        "\n",
        "      Args:\n",
        "        dp_sum_query: DPQuery object, specifying differential privacy\n",
        "          mechanism to use.\n",
        "        num_microbatches: How many microbatches into which the minibatch is\n",
        "          split. If None, will default to the size of the minibatch, and\n",
        "          per-example gradients will be computed.\n",
        "        unroll_microbatches: If true, processes microbatches within a Python\n",
        "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
        "          raises an exception.\n",
        "      \"\"\"\n",
        "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
        "      self._dp_sum_query = dp_sum_query\n",
        "      self._num_microbatches = num_microbatches\n",
        "      self._global_state = self._dp_sum_query.initial_global_state()\n",
        "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
        "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
        "      # may cause an OOM error.\n",
        "      self._unroll_microbatches = unroll_microbatches\n",
        "\n",
        "    def compute_gradients(self,\n",
        "                          loss,\n",
        "                          var_list,\n",
        "                          gate_gradients=GATE_OP,\n",
        "                          aggregation_method=None,\n",
        "                          colocate_gradients_with_ops=False,\n",
        "                          grad_loss=None,\n",
        "                          gradient_tape=None,\n",
        "                          curr_noise_mult=0,\n",
        "                          curr_norm_clip=1):\n",
        "\n",
        "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
        "                                                           curr_norm_clip*curr_noise_mult)\n",
        "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
        "                                                                curr_norm_clip*curr_noise_mult)\n",
        "      \n",
        "\n",
        "      # TF is running in Eager mode, check we received a vanilla tape.\n",
        "      if not gradient_tape:\n",
        "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
        "\n",
        "      vector_loss = loss()\n",
        "      if self._num_microbatches is None:\n",
        "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
        "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
        "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
        "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
        "\n",
        "      def process_microbatch(i, sample_state):\n",
        "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
        "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
        "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
        "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
        "        return sample_state\n",
        "    \n",
        "      for idx in range(self._num_microbatches):\n",
        "        sample_state = process_microbatch(idx, sample_state)\n",
        "\n",
        "      if curr_noise_mult > 0:\n",
        "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
        "      else:\n",
        "        grad_sums = sample_state\n",
        "\n",
        "      def normalize(v):\n",
        "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
        "\n",
        "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
        "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
        "    \n",
        "      return grads_and_vars\n",
        "\n",
        "  return DPOptimizerClass\n",
        "\n",
        "\n",
        "def make_gaussian_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
        "\n",
        "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
        "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l2_norm_clip,\n",
        "        noise_multiplier,\n",
        "        num_microbatches=None,\n",
        "        ledger=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "        **kwargs):\n",
        "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
        "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
        "\n",
        "      if ledger:\n",
        "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
        "                                                      ledger=ledger)\n",
        "\n",
        "      super(DPGaussianOptimizerClass, self).__init__(\n",
        "          dp_sum_query,\n",
        "          num_microbatches,\n",
        "          unroll_microbatches,\n",
        "          *args,\n",
        "          **kwargs)\n",
        "\n",
        "    @property\n",
        "    def ledger(self):\n",
        "      return self._dp_sum_query.ledger\n",
        "\n",
        "  return DPGaussianOptimizerClass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4G0cXs0umr1"
      },
      "source": [
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6et2zT5orb"
      },
      "source": [
        "# load data\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()\n",
        "\n",
        "# normalize data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# split data into validation and training set\n",
        "validation_images = train_images[:5000]\n",
        "validation_labels = train_labels[:5000]\n",
        "train_images = train_images[5000:]\n",
        "train_labels = train_labels[5000:]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "optimizer = DPGradientDescentGaussianOptimizer_NEW(\n",
        "    l2_norm_clip=1.0,\n",
        "    noise_multiplier=1.1,\n",
        "    num_microbatches=250,\n",
        "    learning_rate=0.15)\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# create data generator\n",
        "data_generator = ImageDataGenerator(width_shift_range=0.1,\n",
        "                                    height_shift_range=0.1,\n",
        "                                    horizontal_flip=True)\n",
        "\n",
        "# prepare iterator\n",
        "train_iterator = data_generator.flow(train_images, train_labels, batch_size=64)\n",
        "\n",
        "# prepare validation iterator\n",
        "test_iterator = data_generator.flow(test_images, test_labels, batch_size=64)\n",
        "\n",
        "# prepare validation iterator\n",
        "validation_iterator = data_generator.flow(validation_images, validation_labels, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q3KbsycQtux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a65294-3274-4b44-fe5d-439884ae13d7"
      },
      "source": [
        "#compute epsilon\n",
        "epsilon, delta = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000, batch_size=120, noise_multiplier=0.9, epochs=5, delta=1e-5)\n",
        "\n",
        "# train model\n",
        "print(\"Epsilon: \", epsilon)\n",
        "\n",
        "# train model\n",
        "model.fit(train_iterator,\n",
        "          epochs=5,\n",
        "          validation_data=validation_iterator,\n",
        "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# evaluate model\n",
        "test_loss, test_acc = model.evaluate(test_iterator, verbose=2)\n",
        "\n",
        "#train evaluator\n",
        "train_loss, train_acc = model.evaluate(train_iterator, verbose=2)\n",
        "\n",
        "\n",
        "# print results\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "# save model\n",
        "model.save('/content/drive/MyDrive/SPAI_projectphase2/cifar100_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DP-SGD with sampling rate = 0.2% and noise_multiplier = 0.9 iterated over 2500 steps satisfies differential privacy with eps = 1.17 and delta = 1e-05.\n",
            "The optimal RDP order is 9.0.\n",
            "Epsilon:  1.168712467806508\n",
            "Epoch 1/5\n",
            "703/704 [============================>.] - ETA: 0s - loss: 4.2837 - accuracy: 0.0156WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdf31c73b90> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdf31c73b90> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "704/704 [==============================] - 46s 64ms/step - loss: 4.2834 - accuracy: 0.0156 - val_loss: 3.9733 - val_accuracy: 0.0084\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 45s 64ms/step - loss: 3.8109 - accuracy: 0.0132 - val_loss: 3.6517 - val_accuracy: 0.0118\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 45s 64ms/step - loss: 3.5528 - accuracy: 0.0118 - val_loss: 3.4353 - val_accuracy: 0.0054\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 45s 63ms/step - loss: 3.3400 - accuracy: 0.0107 - val_loss: 3.2660 - val_accuracy: 0.0028\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 45s 63ms/step - loss: 3.1913 - accuracy: 0.0108 - val_loss: 5.3998 - val_accuracy: 0.0282\n",
            "157/157 - 5s - loss: 5.3845 - accuracy: 0.0324 - 5s/epoch - 35ms/step\n",
            "704/704 - 24s - loss: 5.3530 - accuracy: 0.0312 - 24s/epoch - 35ms/step\n",
            "\n",
            "Test accuracy: 0.03240000084042549\n",
            "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
          ]
        }
      ]
    }
  ]
}