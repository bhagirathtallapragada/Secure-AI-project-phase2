{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multipartysplitlearning_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagirathtallapragada/Secure-AI-project-phase2/blob/main/multipartysplitlearning_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGg03zGlnlFd"
      },
      "source": [
        "# This notebook is an implementation to conduct Split learning using the CIFAR100 dataset with the following settings:\n",
        "## Cases you need to consider: \n",
        "1) Every party has instances of every class, \n",
        "2) Every party only holds instances of one class. \n",
        "\n",
        "Reporting training and test performance in the above settings with 10, 20, 50, 100 parties. Also report how fast the training converges and time it\n",
        "take to train with different number of parties involved. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugEJY0dbnLcY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee5cea5-32c0-43ef-bd02-23b7d2f9eb05"
      },
      "source": [
        "# mounting the drive to store the results\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #, force_remount = True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import sys\n",
        "# sys.path.append()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "julbQdcPprR6",
        "outputId": "5fb49a27-0529-4b45-abc6-5bcc4811acb4"
      },
      "source": [
        "!pip install syft==0.2.5 --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 369 kB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 46.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 40.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 484 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.0 kB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 200 kB 68.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 43.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.7 MB/s \n",
            "\u001b[?25h  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.4.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 4.5.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\n",
            "bokeh 2.3.3 requires tornado>=5.1, but you have tornado 4.5.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSGlHqePo0Bi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from time import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torchvision.transforms as trans\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split,ConcatDataset\n",
        "from torch import nn, optim\n",
        "import syft as sy\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1NOY4mzDpvw"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSfhQzztEJp-",
        "outputId": "35e49eff-13c6-497b-fdb3-71a3589210cc"
      },
      "source": [
        "train_data = CIFAR100(download=True,root=\"/content/drive/MyDrive/SPAI_projectphase2/data\",transform=transform)\n",
        "test_data = CIFAR100(root=\"/content/drive/MyDrive/SPAI_projectphase2/data\",train=False,transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRf1jUEoEB2s",
        "outputId": "3f5f3929-4370-48e7-e671-99db8b6b1417"
      },
      "source": [
        "for image,label in train_data:\n",
        "    print(\"Image shape: \",image.shape)\n",
        "    # print(\"Image tensor: \", image)\n",
        "    print(\"Label: \", label)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape:  torch.Size([3, 32, 32])\n",
            "Label:  19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS9z-azHFvty",
        "outputId": "a8e70c2b-6194-4448-d935-9001b7305ec1"
      },
      "source": [
        "# Number of classes should be 100\n",
        "len(train_data.classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq6PpF4NTJzE"
      },
      "source": [
        "## processing the data as needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8oSzKlbTNkN"
      },
      "source": [
        "def extract_classes( data, labels,n):\n",
        "    data_cl=[]\n",
        "    y=[]\n",
        "    for i in range(n):\n",
        "            data_cl.append(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1) ][ : ])\n",
        "            y.extend(np.full((data_cl[i].shape[0]), i, dtype=int))\n",
        "\n",
        "\n",
        "    x = np.vstack( (data_cl) )\n",
        "    y = np.array(y)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jor2_vekTWog",
        "outputId": "8e6b25fb-4762-4461-934a-2510d98b01af"
      },
      "source": [
        "x_train,y_train = extract_classes(train_data.data,np.array(train_data.targets),n=100)\n",
        "x_train.shape,y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p3mRnhCTpUb",
        "outputId": "20a4fc53-8950-4830-ce02-88a9f713fcc0"
      },
      "source": [
        "print(y_train)\n",
        "\n",
        "train_data.data = x_train\n",
        "train_data.targets = y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0 ... 99 99 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUy3eGWT8f4",
        "outputId": "c84f629b-d255-4919-a9e2-1486ea84db07"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=False)\n",
        "torch.manual_seed(0)\n",
        "trainloader.dataset.targets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0, ..., 99, 99, 99])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doj8_DtlKsJi"
      },
      "source": [
        "## Define a list of total 100 layers that we will be distributing between parties based on number of parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-4tff8LKqY1",
        "outputId": "c7957f73-8bef-44fe-9806-fe2ad98b3010"
      },
      "source": [
        "total = 100\n",
        "\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "\n",
        "layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
        "\n",
        "for i in range(1, int(total/2)-1):\n",
        "  if (i%2 != 0):\n",
        "    layers.extend([nn.Linear(hidden_sizes[0], hidden_sizes[1]), nn.ReLU()])\n",
        "  else:\n",
        "    layers.extend([nn.Linear(hidden_sizes[1], hidden_sizes[0]), nn.ReLU()])\n",
        "\n",
        "# the final layer\n",
        "layers.extend([nn.Linear(hidden_sizes[0], output_size), nn.LogSoftmax(dim=1)])\n",
        "\n",
        "print(len(layers))\n",
        "layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Linear(in_features=3072, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=640, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=640, out_features=128, bias=True),\n",
              " ReLU(),\n",
              " Linear(in_features=128, out_features=100, bias=True),\n",
              " LogSoftmax()]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zouro_jSidFe"
      },
      "source": [
        "## Defining the splitlearning class to implement multiparty split learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peRNTcSncCnN"
      },
      "source": [
        "class SplitNN:\n",
        "    def __init__(self, models, optimizers):\n",
        "        self.models = models\n",
        "        self.optimizers = optimizers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        a = []\n",
        "        remote_a = []\n",
        "        \n",
        "        a.append(models[0](x))\n",
        "        if a[-1].location == models[1].location:\n",
        "            remote_a.append(a[-1].detach().requires_grad_())\n",
        "        else:\n",
        "            remote_a.append(a[-1].detach().move(models[1].location).requires_grad_())\n",
        "\n",
        "        i=1    \n",
        "        while i < (len(models)-1):\n",
        "            \n",
        "            a.append(models[i](remote_a[-1]))\n",
        "            if a[-1].location == models[i+1].location:\n",
        "                remote_a.append(a[-1].detach().requires_grad_())\n",
        "            else:\n",
        "                remote_a.append(a[-1].detach().move(models[i+1].location).requires_grad_())\n",
        "            \n",
        "            i+=1\n",
        "        \n",
        "        a.append(models[i](remote_a[-1]))\n",
        "        self.a = a\n",
        "        self.remote_a = remote_a\n",
        "        \n",
        "        return a[-1]\n",
        "    \n",
        "    def backward(self):\n",
        "        a=self.a\n",
        "        remote_a=self.remote_a\n",
        "        optimizers = self.optimizers\n",
        "        \n",
        "        i= len(models)-2   \n",
        "        while i > -1:\n",
        "            if remote_a[i].location == a[i].location:\n",
        "                grad_a = remote_a[i].grad.copy()\n",
        "            else:\n",
        "                grad_a = remote_a[i].grad.copy().move(a[i].location)\n",
        "            a[i].backward(grad_a)\n",
        "            i-=1\n",
        "\n",
        "    \n",
        "    def zero_grads(self):\n",
        "        for opt in optimizers:\n",
        "            opt.zero_grad()\n",
        "        \n",
        "    def step(self):\n",
        "        for opt in optimizers:\n",
        "            opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsDcu6ToRo7O"
      },
      "source": [
        "def train(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    #3) Figure out how much we missed by\n",
        "    criterion = nn.NLLLoss()\n",
        "    loss = criterion(pred, target)\n",
        "  \n",
        "    #4) Backprop the loss on the end layer\n",
        "    loss.backward()\n",
        "    \n",
        "    #5) Feed Gradients backward through the network\n",
        "    splitNN.backward()\n",
        "    \n",
        "    #6) Change the weights\n",
        "    splitNN.step()\n",
        "    \n",
        "    return loss,pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQGHZqICcimp"
      },
      "source": [
        "def test(x, target, splitNN):\n",
        "    \n",
        "    #1) Zero our grads\n",
        "    # splitNN.zero_grads()\n",
        "    \n",
        "    #2) Make a prediction\n",
        "    pred = splitNN.forward(x)\n",
        "    \n",
        "    #3) Figure out how much we missed by\n",
        "    # criterion = nn.NLLLoss()\n",
        "    # loss = criterion(pred, target)\n",
        "    \n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-fiLwyUR3w6"
      },
      "source": [
        "## Multiparty split learning with 10 parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3g12JXLaceT"
      },
      "source": [
        "n_instances = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pv9s09Oijv"
      },
      "source": [
        "### creating a list of sequential models by building it from above defined layers list based on number of parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaSm-rnVOh7q",
        "outputId": "0a6f94ce-2943-49eb-9d67-554e89199a2c"
      },
      "source": [
        "models = []\n",
        "i=0\n",
        "while(i < 100):\n",
        "  # each sequential model should have 100/ n_instances layers\n",
        "  models.append(nn.Sequential(*layers[i:i+int(100/n_instances)]))\n",
        "  i+=int(100/n_instances)\n",
        "\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# This should now show a list of 10 models each with 10 layers\n",
        "models  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (9): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (5): ReLU()\n",
              "   (6): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (7): ReLU()\n",
              "   (8): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (9): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUlRn8IuoFse",
        "outputId": "f0d3e003-d165-4b0b-db91-080046337500"
      },
      "source": [
        "alices = []\n",
        "for i in range(n_instances):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)\n",
        "alices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNHkKRzLZqv"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZTYURdAegq1"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ehKuIk7Ucowo",
        "outputId": "6e6ceb2e-6f79-40bc-9456-713f7ec7c69d"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice10'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFIlJFAFcxl7",
        "outputId": "f1187685-98d1-4413-b071-9f7f148ec9bc"
      },
      "source": [
        "%%time\n",
        "epochs = 3\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss, pred = train(images, labels, splitNN)\n",
        "\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "\n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n",
            "Epoch 2 - Training loss: 3.262223482131958 Training Accuracy: 0.20076726342711\n",
            "CPU times: user 9min 59s, sys: 1min 4s, total: 11min 3s\n",
            "Wall time: 11min 3s\n",
            "Parser   : 143 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dfX4D4yc6Zi",
        "outputId": "9bbfb26f-3389-4a82-8168-bd5219a6cedc"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(test_data,shuffle=False)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f428293de30>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsFmhL2vdCYG",
        "outputId": "e43fe438-0934-4426-82c4-43646588a403"
      },
      "source": [
        "%%time\n",
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    images = images.send(models[0].location)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    labels = labels.send(models[-1].location)\n",
        "    pred = test(images, labels, splitNN)\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    train_acc = torch.sum(pred == labels).get().item()\n",
        "    # acc_c = train_acc/len(labels)\n",
        "    # print(images.shape,labels.shape)\n",
        "    acc +=train_acc\n",
        "    # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n",
            "CPU times: user 9min 13s, sys: 47.2 s, total: 10min\n",
            "Wall time: 9min 59s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLno5cXOjwNw"
      },
      "source": [
        "## Multiparty split learning with 20 parties (may need to restart runtime before implementing this section)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkczkXy8VoGn"
      },
      "source": [
        "n_instances = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfFBUzXlVoG5"
      },
      "source": [
        "### creating a list of sequential models by building it from above defined layers list based on number of parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPQsXiqgVoG5",
        "outputId": "7dbdb5e4-95ef-4c2a-c58a-fa058615f266"
      },
      "source": [
        "models = []\n",
        "i=0\n",
        "while(i < 100):\n",
        "  # each sequential model should have 100/ n_instances layers\n",
        "  models.append(nn.Sequential(*layers[i:i+int(100/n_instances)]))\n",
        "  i+=int(100/n_instances)\n",
        "\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# This should now show a list of 20 models each with 5 layers\n",
        "models  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=640, out_features=128, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (4): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              "   (2): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (3): ReLU()\n",
              "   (4): Linear(in_features=128, out_features=640, bias=True)\n",
              " ), Sequential(\n",
              "   (0): ReLU()\n",
              "   (1): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (2): ReLU()\n",
              "   (3): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (4): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUQxDVhzjwNy",
        "outputId": "2d0dfe12-4586-4ca1-99f4-ac2a232deeb7"
      },
      "source": [
        "alices = []\n",
        "for i in range(n_instances):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)\n",
        "alices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>,\n",
              " <VirtualWorker id:alice11 #objects:6>,\n",
              " <VirtualWorker id:alice12 #objects:6>,\n",
              " <VirtualWorker id:alice13 #objects:6>,\n",
              " <VirtualWorker id:alice14 #objects:6>,\n",
              " <VirtualWorker id:alice15 #objects:6>,\n",
              " <VirtualWorker id:alice16 #objects:6>,\n",
              " <VirtualWorker id:alice17 #objects:6>,\n",
              " <VirtualWorker id:alice18 #objects:6>,\n",
              " <VirtualWorker id:alice19 #objects:6>,\n",
              " <VirtualWorker id:alice20 #objects:6>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaRdf0a9jwNy"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOmHzcYdjwNy"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dmbb-o7YjwNy",
        "outputId": "1dedbb1e-24f5-465e-f886-ea897464dd6f"
      },
      "source": [
        "models[-1].location.id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alice20'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek-GO-W5jwNz",
        "outputId": "66d1b3d5-bf24-4b30-c162-4d917af51cd2"
      },
      "source": [
        "%%time\n",
        "epochs = 3\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss, pred = train(images, labels, splitNN)\n",
        "\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "\n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n",
            "Epoch 2 - Training loss: 3.262223482131958 Training Accuracy: 0.20076726342711\n",
            "CPU times: user 11min 24s, sys: 1min 45s, total: 13min 10s\n",
            "Wall time: 13min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FigSsHXujwNz",
        "outputId": "31cde24d-c4ad-4af0-8ebf-b8d4da840a2e"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(test_data,shuffle=False)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1d6b7f7e30>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy4Y7Bc1jwNz",
        "outputId": "cbeaa74f-6ce9-49e9-eb3d-cb96969f0290"
      },
      "source": [
        "%%time\n",
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    images = images.send(models[0].location)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    labels = labels.send(models[-1].location)\n",
        "    pred = test(images, labels, splitNN)\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    train_acc = torch.sum(pred == labels).get().item()\n",
        "    # acc_c = train_acc/len(labels)\n",
        "    # print(images.shape,labels.shape)\n",
        "    acc +=train_acc\n",
        "    # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n",
            "CPU times: user 11min 21s, sys: 1min 17s, total: 12min 39s\n",
            "Wall time: 12min 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5foPJWZQh2iO"
      },
      "source": [
        "## Multiparty split learning with 50 parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_hArNCpY_gJ"
      },
      "source": [
        "n_instances = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNKKv4fNZ-HD"
      },
      "source": [
        "### creating a list of sequential models by building it from above defined layers list based on number of parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bV9E_q8Y_gQ",
        "outputId": "fcef92be-0c14-406e-8748-9582a0899b71"
      },
      "source": [
        "models = []\n",
        "i=0\n",
        "while(i < 100):\n",
        "  # each sequential model should have 100/ n_instances layers\n",
        "  models.append(nn.Sequential(*layers[i:i+int(100/n_instances)]))\n",
        "  i+=int(100/n_instances)\n",
        "\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# This should now show a list of 50 models each with 2 layers\n",
        "models  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpFOUAP2h2iZ",
        "outputId": "f6e273a1-84ce-4f40-de05-3618751f6341"
      },
      "source": [
        "alices = []\n",
        "for i in range(n_instances):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)\n",
        "alices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>,\n",
              " <VirtualWorker id:alice11 #objects:6>,\n",
              " <VirtualWorker id:alice12 #objects:6>,\n",
              " <VirtualWorker id:alice13 #objects:6>,\n",
              " <VirtualWorker id:alice14 #objects:6>,\n",
              " <VirtualWorker id:alice15 #objects:6>,\n",
              " <VirtualWorker id:alice16 #objects:6>,\n",
              " <VirtualWorker id:alice17 #objects:6>,\n",
              " <VirtualWorker id:alice18 #objects:6>,\n",
              " <VirtualWorker id:alice19 #objects:6>,\n",
              " <VirtualWorker id:alice20 #objects:6>,\n",
              " <VirtualWorker id:alice21 #objects:6>,\n",
              " <VirtualWorker id:alice22 #objects:6>,\n",
              " <VirtualWorker id:alice23 #objects:6>,\n",
              " <VirtualWorker id:alice24 #objects:6>,\n",
              " <VirtualWorker id:alice25 #objects:6>,\n",
              " <VirtualWorker id:alice26 #objects:6>,\n",
              " <VirtualWorker id:alice27 #objects:6>,\n",
              " <VirtualWorker id:alice28 #objects:6>,\n",
              " <VirtualWorker id:alice29 #objects:6>,\n",
              " <VirtualWorker id:alice30 #objects:6>,\n",
              " <VirtualWorker id:alice31 #objects:6>,\n",
              " <VirtualWorker id:alice32 #objects:6>,\n",
              " <VirtualWorker id:alice33 #objects:6>,\n",
              " <VirtualWorker id:alice34 #objects:6>,\n",
              " <VirtualWorker id:alice35 #objects:6>,\n",
              " <VirtualWorker id:alice36 #objects:6>,\n",
              " <VirtualWorker id:alice37 #objects:6>,\n",
              " <VirtualWorker id:alice38 #objects:6>,\n",
              " <VirtualWorker id:alice39 #objects:6>,\n",
              " <VirtualWorker id:alice40 #objects:6>,\n",
              " <VirtualWorker id:alice41 #objects:6>,\n",
              " <VirtualWorker id:alice42 #objects:6>,\n",
              " <VirtualWorker id:alice43 #objects:6>,\n",
              " <VirtualWorker id:alice44 #objects:6>,\n",
              " <VirtualWorker id:alice45 #objects:6>,\n",
              " <VirtualWorker id:alice46 #objects:6>,\n",
              " <VirtualWorker id:alice47 #objects:6>,\n",
              " <VirtualWorker id:alice48 #objects:6>,\n",
              " <VirtualWorker id:alice49 #objects:6>,\n",
              " <VirtualWorker id:alice50 #objects:6>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukDA-YXzh2iZ"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J2gWsRMh2ia"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqwPE3U2h2ia",
        "outputId": "8fcdf65f-2b7f-442a-9898-a07c86b4ebcb"
      },
      "source": [
        "%%time\n",
        "epochs = 3\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.026189804077148 Training Accuracy: 0.07217071611253197\n",
            "Epoch 1 - Training loss: 3.626110553741455 Training Accuracy: 0.1257992327365729\n",
            "Epoch 2 - Training loss: 3.262223482131958 Training Accuracy: 0.20076726342711\n",
            "CPU times: user 16min 9s, sys: 5min 20s, total: 21min 29s\n",
            "Wall time: 21min 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFAR_L1ah2ia",
        "outputId": "831f412a-bff5-4d1f-9ec1-4429079f44e6"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(test_data,shuffle=False)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f86f922d510>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12mtTFVbh2ia",
        "outputId": "a3ac4578-c9f2-4a00-a9c5-862076c615a7"
      },
      "source": [
        "%%time\n",
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    images = images.send(models[0].location)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    labels = labels.send(models[-1].location)\n",
        "    pred = test(images, labels, splitNN)\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    train_acc = torch.sum(pred == labels).get().item()\n",
        "    # acc_c = train_acc/len(labels)\n",
        "    # print(images.shape,labels.shape)\n",
        "    acc +=train_acc\n",
        "    # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n",
            "CPU times: user 25min 7s, sys: 41min 30s, total: 1h 6min 38s\n",
            "Wall time: 1h 6min 35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OI9HAY3jgDd"
      },
      "source": [
        "## Multiparty split learning with 100 parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J49etwdmaEP5"
      },
      "source": [
        "n_instances = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nzMzh2_aEP6"
      },
      "source": [
        "### creating a list of sequential models by building it from above defined layers list based on number of parties"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8JyhuflRsui",
        "outputId": "0f81d43c-8bd5-4ed0-b50b-1a498175a643"
      },
      "source": [
        "epochs = 3\n",
        "\n",
        "# Define our model segments\n",
        "models = []\n",
        "input_size = 3072\n",
        "hidden_sizes = [128, 640]\n",
        "output_size = 100\n",
        "inp = input_size\n",
        "out = hidden_sizes[0]\n",
        "k=0\n",
        "for i in range(99):\n",
        "\n",
        "  models.append(nn.Sequential(\n",
        "              nn.Linear(inp, out),\n",
        "              nn.ReLU(),\n",
        "  ))\n",
        "  inp = hidden_sizes[k]\n",
        "  if k==0:\n",
        "      k=1\n",
        "  else:\n",
        "      k=0\n",
        "  out = hidden_sizes[k]\n",
        "\n",
        "models.append(nn.Sequential(\n",
        "                nn.Linear(inp, output_size),\n",
        "                nn.LogSoftmax(dim=1)\n",
        "    ))\n",
        "models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sequential(\n",
              "   (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=640, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=640, out_features=128, bias=True)\n",
              "   (1): ReLU()\n",
              " ), Sequential(\n",
              "   (0): Linear(in_features=128, out_features=100, bias=True)\n",
              "   (1): LogSoftmax()\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXH-g28IaEP6"
      },
      "source": [
        "# models = []\n",
        "# i=0\n",
        "# while(i < 100):\n",
        "#   # each sequential model should have 100/ n_instances layers\n",
        "#   models.append(nn.Sequential(*layers[i:i+(100/n_instances)]))\n",
        "#   i+=n_instances\n",
        "\n",
        "optimizers = [\n",
        "    optim.SGD(model.parameters(), lr=0.03,)\n",
        "    for model in models\n",
        "]\n",
        "\n",
        "# # This should now show a list of 100 models each with 1 layers\n",
        "# models  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGAwBbu3WYF7",
        "outputId": "dcd65b59-18d4-4252-fb6b-7f5199f40b53"
      },
      "source": [
        "len(models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpgUY0YHjgDe",
        "outputId": "2452cf01-8a7f-4b82-8046-a0d1854b71b8"
      },
      "source": [
        "alices = []\n",
        "for i in range(n_instances):\n",
        "    alices.append(sy.VirtualWorker(hook, id=\"alice\"+str(i+1)))\n",
        "\n",
        "workers = tuple(alices)\n",
        "alices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<VirtualWorker id:alice1 #objects:6>,\n",
              " <VirtualWorker id:alice2 #objects:6>,\n",
              " <VirtualWorker id:alice3 #objects:6>,\n",
              " <VirtualWorker id:alice4 #objects:6>,\n",
              " <VirtualWorker id:alice5 #objects:6>,\n",
              " <VirtualWorker id:alice6 #objects:6>,\n",
              " <VirtualWorker id:alice7 #objects:6>,\n",
              " <VirtualWorker id:alice8 #objects:6>,\n",
              " <VirtualWorker id:alice9 #objects:6>,\n",
              " <VirtualWorker id:alice10 #objects:6>,\n",
              " <VirtualWorker id:alice11 #objects:6>,\n",
              " <VirtualWorker id:alice12 #objects:6>,\n",
              " <VirtualWorker id:alice13 #objects:6>,\n",
              " <VirtualWorker id:alice14 #objects:6>,\n",
              " <VirtualWorker id:alice15 #objects:6>,\n",
              " <VirtualWorker id:alice16 #objects:6>,\n",
              " <VirtualWorker id:alice17 #objects:6>,\n",
              " <VirtualWorker id:alice18 #objects:6>,\n",
              " <VirtualWorker id:alice19 #objects:6>,\n",
              " <VirtualWorker id:alice20 #objects:6>,\n",
              " <VirtualWorker id:alice21 #objects:6>,\n",
              " <VirtualWorker id:alice22 #objects:6>,\n",
              " <VirtualWorker id:alice23 #objects:6>,\n",
              " <VirtualWorker id:alice24 #objects:6>,\n",
              " <VirtualWorker id:alice25 #objects:6>,\n",
              " <VirtualWorker id:alice26 #objects:6>,\n",
              " <VirtualWorker id:alice27 #objects:6>,\n",
              " <VirtualWorker id:alice28 #objects:6>,\n",
              " <VirtualWorker id:alice29 #objects:6>,\n",
              " <VirtualWorker id:alice30 #objects:6>,\n",
              " <VirtualWorker id:alice31 #objects:6>,\n",
              " <VirtualWorker id:alice32 #objects:6>,\n",
              " <VirtualWorker id:alice33 #objects:6>,\n",
              " <VirtualWorker id:alice34 #objects:6>,\n",
              " <VirtualWorker id:alice35 #objects:6>,\n",
              " <VirtualWorker id:alice36 #objects:6>,\n",
              " <VirtualWorker id:alice37 #objects:6>,\n",
              " <VirtualWorker id:alice38 #objects:6>,\n",
              " <VirtualWorker id:alice39 #objects:6>,\n",
              " <VirtualWorker id:alice40 #objects:6>,\n",
              " <VirtualWorker id:alice41 #objects:6>,\n",
              " <VirtualWorker id:alice42 #objects:6>,\n",
              " <VirtualWorker id:alice43 #objects:6>,\n",
              " <VirtualWorker id:alice44 #objects:6>,\n",
              " <VirtualWorker id:alice45 #objects:6>,\n",
              " <VirtualWorker id:alice46 #objects:6>,\n",
              " <VirtualWorker id:alice47 #objects:6>,\n",
              " <VirtualWorker id:alice48 #objects:6>,\n",
              " <VirtualWorker id:alice49 #objects:6>,\n",
              " <VirtualWorker id:alice50 #objects:6>,\n",
              " <VirtualWorker id:alice51 #objects:6>,\n",
              " <VirtualWorker id:alice52 #objects:6>,\n",
              " <VirtualWorker id:alice53 #objects:6>,\n",
              " <VirtualWorker id:alice54 #objects:6>,\n",
              " <VirtualWorker id:alice55 #objects:6>,\n",
              " <VirtualWorker id:alice56 #objects:6>,\n",
              " <VirtualWorker id:alice57 #objects:6>,\n",
              " <VirtualWorker id:alice58 #objects:6>,\n",
              " <VirtualWorker id:alice59 #objects:6>,\n",
              " <VirtualWorker id:alice60 #objects:6>,\n",
              " <VirtualWorker id:alice61 #objects:6>,\n",
              " <VirtualWorker id:alice62 #objects:6>,\n",
              " <VirtualWorker id:alice63 #objects:6>,\n",
              " <VirtualWorker id:alice64 #objects:6>,\n",
              " <VirtualWorker id:alice65 #objects:6>,\n",
              " <VirtualWorker id:alice66 #objects:6>,\n",
              " <VirtualWorker id:alice67 #objects:6>,\n",
              " <VirtualWorker id:alice68 #objects:6>,\n",
              " <VirtualWorker id:alice69 #objects:6>,\n",
              " <VirtualWorker id:alice70 #objects:6>,\n",
              " <VirtualWorker id:alice71 #objects:6>,\n",
              " <VirtualWorker id:alice72 #objects:6>,\n",
              " <VirtualWorker id:alice73 #objects:6>,\n",
              " <VirtualWorker id:alice74 #objects:6>,\n",
              " <VirtualWorker id:alice75 #objects:6>,\n",
              " <VirtualWorker id:alice76 #objects:6>,\n",
              " <VirtualWorker id:alice77 #objects:6>,\n",
              " <VirtualWorker id:alice78 #objects:6>,\n",
              " <VirtualWorker id:alice79 #objects:6>,\n",
              " <VirtualWorker id:alice80 #objects:6>,\n",
              " <VirtualWorker id:alice81 #objects:6>,\n",
              " <VirtualWorker id:alice82 #objects:6>,\n",
              " <VirtualWorker id:alice83 #objects:6>,\n",
              " <VirtualWorker id:alice84 #objects:6>,\n",
              " <VirtualWorker id:alice85 #objects:6>,\n",
              " <VirtualWorker id:alice86 #objects:6>,\n",
              " <VirtualWorker id:alice87 #objects:6>,\n",
              " <VirtualWorker id:alice88 #objects:6>,\n",
              " <VirtualWorker id:alice89 #objects:6>,\n",
              " <VirtualWorker id:alice90 #objects:6>,\n",
              " <VirtualWorker id:alice91 #objects:6>,\n",
              " <VirtualWorker id:alice92 #objects:6>,\n",
              " <VirtualWorker id:alice93 #objects:6>,\n",
              " <VirtualWorker id:alice94 #objects:6>,\n",
              " <VirtualWorker id:alice95 #objects:6>,\n",
              " <VirtualWorker id:alice96 #objects:6>,\n",
              " <VirtualWorker id:alice97 #objects:6>,\n",
              " <VirtualWorker id:alice98 #objects:6>,\n",
              " <VirtualWorker id:alice99 #objects:6>,\n",
              " <VirtualWorker id:alice100 #objects:6>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEisiXG_jgDe"
      },
      "source": [
        "# Send Model Segments to starting locations\n",
        "model_locations = alices\n",
        "for model, location in zip(models, model_locations):\n",
        "    model.send(location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfdCMpzXjgDe"
      },
      "source": [
        "splitNN =  SplitNN(models, optimizers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvaWI0UljgDe",
        "outputId": "32e7c0bb-8f07-4477-ff1d-62d0fd585221"
      },
      "source": [
        "epochs = 3\n",
        "for i in range(epochs):\n",
        "    running_loss = 0\n",
        "    acc = 0\n",
        "    for images, labels in trainloader:\n",
        "        loss = 0\n",
        "        acc_c = 0\n",
        "        images = images.send(models[0].location)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        labels = labels.send(models[-1].location)\n",
        "        loss,pred = train(images, labels, splitNN)\n",
        "        # print(pred.shape)\n",
        "        # print(labels.shape)\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        train_acc = torch.sum(pred == labels).get().item()\n",
        "        acc_c = train_acc/len(labels)\n",
        "        # print(train_acc.get()) \n",
        "        running_loss += loss.get()\n",
        "        acc +=acc_c\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Training loss: {} Training Accuracy: {}\".format(i, running_loss/len(trainloader),acc/len(trainloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Training loss: 4.175104141235352 Training Accuracy: 0.050831202046035806\n",
            "Epoch 1 - Training loss: 3.870326042175293 Training Accuracy: 0.08104219948849105\n",
            "Epoch 2 - Training loss: 3.449857234954834 Training Accuracy: 0.15361253196930946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmqe46iwBVMZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlFqeTJnjgDe",
        "outputId": "a57a5531-0848-4b3e-b0b3-95d84dae624d"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(test_data,shuffle=False)\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc0fefad390>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx0ViyGUjgDe",
        "outputId": "aae7f5ef-8b1a-45ed-d707-a2526e0d02c4"
      },
      "source": [
        "%%time\n",
        "# loss=0\n",
        "acc_c = 0\n",
        "acc = 0\n",
        "with torch.no_grad():\n",
        "  for images, labels in testloader:\n",
        "    images = images.send(models[0].location)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    labels = labels.send(models[-1].location)\n",
        "    pred = test(images, labels, splitNN)\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    train_acc = torch.sum(pred == labels).get().item()\n",
        "    # acc_c = train_acc/len(labels)\n",
        "    # print(images.shape,labels.shape)\n",
        "    acc +=train_acc\n",
        "    # running_loss += loss.get()\n",
        "print(\"Testing accuracy: {}\".format(acc/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy: 0.01\n"
          ]
        }
      ]
    }
  ]
}