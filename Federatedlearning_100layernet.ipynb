{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federatedlearning_100layernet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3HVaPn0_u7pI",
        "aCDcKszC6u9G",
        "M3C8S-3t6u9O",
        "ilRqeeKO6u9X",
        "05kAABtz6u9e"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagirathtallapragada/Secure-AI-project-phase2/blob/main/Federatedlearning_100layernet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjYCLnY600Y2"
      },
      "source": [
        "This notebook is an implementation to conduct federated learning using the CIFAR100 dataset with the following settings:\n",
        "Cases you need to consider:\n",
        "1) Every party has instances of every class, 2) Every party only holds instances of one class.\n",
        "\n",
        "Reporting training and test performance in the above settings with 10, 20, 50, 100 parties. Also report how fast the training converges and time it take to train with different number of parties involved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLqwL9nAp9pN",
        "outputId": "f1d8584c-bf12-4f2e-e2cf-523d1e862eee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Kjrh2K6tJUG"
      },
      "source": [
        "!pip install tensorflow_federated==0.13.1 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugj6pwbYmVjm"
      },
      "source": [
        "# !pip install --upgrade tensorflow --quiet\n",
        "# !pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URz6pyoYqYzL"
      },
      "source": [
        "# !pip uninstall --yes tensorboard tb-nightly\n",
        "# !pip install --quiet --upgrade tensorflow-federated-nightly\n",
        "# !pip install --quiet --upgrade nest-asyncio\n",
        "# !pip install --quiet --upgrade tb-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ksja49SUCxP"
      },
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1-QME01r_Aa"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdNChWdsS1o"
      },
      "source": [
        "import collections\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import utils as np_utils\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import InputLayer, Reshape\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "np.random.seed(0)\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset   \n",
        "torch.backends.cudnn.benchmark=True\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W_zVo50usM7"
      },
      "source": [
        "# Next version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oadc_DyBuyHd"
      },
      "source": [
        "(x_train_100, y_train_100), (x_test_100, y_test_100) = cifar100.load_data()\n",
        "x_train_100 = x_train_100.astype('float32')\n",
        "x_test_100 = x_test_100.astype('float32')\n",
        "x_train_100  /= 255\n",
        "x_test_100 /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5N-btywxP1p"
      },
      "source": [
        "def ext_classes( data, labels,n, cat_y = True):\n",
        "    data_cl=[]\n",
        "    y=[]\n",
        "    # print(data.shape)\n",
        "    for i in range(n):\n",
        "            # print(data[ np.argwhere( labels == i ).reshape(-1) ].shape)\n",
        "            # print(labels.shape)\n",
        "            # print(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1)].shape,data.shape)\n",
        "            data_cl.append(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1) ][ : ])\n",
        "            y.extend(np.full((data_cl[i].shape[0]), i, dtype=int))\n",
        "\n",
        "    # print(np.array(data_cl).shape)\n",
        "    x = np.vstack( (data_cl) )\n",
        "    y = np.array(y)\n",
        "    # print(\"In extract classes function\")\n",
        "    # print(x.shape,y.shape)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khpDewX-wlgw"
      },
      "source": [
        "#extract equal ratio of all classes for training data\n",
        "x_train, y_train = ext_classes( x_train_100, y_train_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHFHvX28iSnN"
      },
      "source": [
        "# for the test case\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGC4Inu_DzrL"
      },
      "source": [
        "x_train = x_train.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtpRazzLin91"
      },
      "source": [
        "x_test = x_test.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKYFpb6iiuFQ",
        "outputId": "0d837e43-bbb6-4ee9-a896-3051c8bbf70e"
      },
      "source": [
        "np.unique(y_test, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              " array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
              "        100, 100, 100, 100, 100, 100, 100, 100, 100]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pL-G5SDcHzB"
      },
      "source": [
        "## 1) Every party only holds instances of one class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgbTAVRWQY4E"
      },
      "source": [
        "#### 1) 10 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61bA-mt5XlPR",
        "outputId": "30352299-7317-47f6-bf30-edbbc3d9bc90"
      },
      "source": [
        "x_train.shape[ 0 ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uTgzDNIdwA8",
        "outputId": "bf2e4c8c-6180-4cd6-f112-d0f53d7753c1"
      },
      "source": [
        "x_train.shape[ 0 ]//100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixgnpkvg3-jP",
        "outputId": "400fec8a-024b-416a-a1cf-124166444f71"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 10 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eVxRHn2tzcb"
      },
      "source": [
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhZXUwuf31r_",
        "outputId": "12adac85-035a-4008-8b3d-e393d33d920e"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "epoch 0 <sparse_categorical_accuracy=0.996999979019165,loss=0.6156036257743835,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 1 <sparse_categorical_accuracy=0.9936666488647461,loss=0.12036780267953873,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 2 <sparse_categorical_accuracy=0.996666669845581,loss=0.021555190905928612,keras_training_time_client_sum_sec=0.0>\n",
            "Training time: 169.59079718589783s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB-qzc_Shf-Q"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oMyPLnQhb6y",
        "outputId": "3502cf23-d834-429c-ce81-917fc627415b"
      },
      "source": [
        "NUM_CLIENTS = 10 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(100,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV425QoSkIGe",
        "outputId": "670e7f93-ac57-4f84-918e-69717c8a29f3"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWbQd37HkO97"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM4qhT3OmtS5",
        "outputId": "bdb90fa2-5b65-45fe-f614-4987998654cb"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sparse_categorical_accuracy=0.10000000149011612,loss=3.0370593070983887,keras_training_time_client_sum_sec=0.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hVQLH5uhcu5"
      },
      "source": [
        "### 2) 20 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir0Bvsv2pPXc"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train, y_train = ext_classes( x_train_100, y_train_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSn8xtVdpPXd"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xqlzpXApPXd"
      },
      "source": [
        "x_test = x_test.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9bU0WCpPXd"
      },
      "source": [
        "x_train = x_train.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiFihS584QVP",
        "outputId": "6d6d6468-3dd5-4b3e-e074-aa284ea5fde9"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 20 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "# shuffle data\n",
        "# idx = np.arange( TOTAL_SAMPLES )\n",
        "# np.random.shuffle( idx )\n",
        "# x_train = x_train[ idx ]\n",
        "# y_train = y_train[ idx ]\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwzFFsCMhcu6"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKIm-gu6m8Mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f690db6c-fcb0-4a16-a4ec-d455d7d0350f"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 <sparse_categorical_accuracy=0.996833324432373,loss=0.5802554488182068,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 1 <sparse_categorical_accuracy=0.9919999837875366,loss=0.10859827697277069,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 2 <sparse_categorical_accuracy=0.9965000152587891,loss=0.021834366023540497,keras_training_time_client_sum_sec=0.0>\n",
            "Training time: 322.7802813053131s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTJ0I2kLm-Eq"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb1K9Iq-m-Eq",
        "outputId": "08e636a3-c594-4d08-8e4d-e35307495c98"
      },
      "source": [
        "NUM_CLIENTS = 20 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(100,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab6294Jxm-Eq",
        "outputId": "b24eb5de-9d0c-472b-f870-73d2f78c3da7"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVpe2-y8m-Er"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyPrbgOm-Er",
        "outputId": "f62ae8ae-55a4-425b-ed79-f92f1256ac99"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sparse_categorical_accuracy=0.05000000074505806,loss=4.143675804138184,keras_training_time_client_sum_sec=0.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUHww-1q5Hg"
      },
      "source": [
        "### 3) 50 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQyiIpbxrGLC"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train, y_train = ext_classes( x_train_100, y_train_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiqGO5mMrGLC"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1eH6QIArGLC"
      },
      "source": [
        "x_test = x_test.reshape(-1,3072)\n",
        "x_train = x_train.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0dDoiQg4pPx",
        "outputId": "f403b9b3-4a0c-45aa-b6f7-0e71dcec8af6"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 50 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTlj0B6OrGLD"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBlktaMmrGLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafb8ef7-7d70-47fe-bda8-ce97086c7459"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 <sparse_categorical_accuracy=0.9967333078384399,loss=0.6145344376564026,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 1 <sparse_categorical_accuracy=0.9870666861534119,loss=0.1530180275440216,keras_training_time_client_sum_sec=0.0>\n",
            "epoch 2 <sparse_categorical_accuracy=0.9947999715805054,loss=0.03344707190990448,keras_training_time_client_sum_sec=0.0>\n",
            "Training time: 810.4315114021301s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf1tTWbprGLD"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWS_KRZKrGLD",
        "outputId": "70cf3510-da62-45b2-a092-8ecd6356a912"
      },
      "source": [
        "NUM_CLIENTS = 50 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(100,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n",
            "(100, 1, 3072)\n",
            "(100, 1, 1)\n",
            "<TensorSliceDataset shapes: ((1, 3072), (1, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cri8T1BurGLD",
        "outputId": "9b3ca1b5-a4c4-4b1d-a9d6-9052d9ca5948"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS-EoQcUrGLD"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYgrBdH6rGLE",
        "outputId": "2fdf1223-77ab-4778-9b38-c1ffa86e8d9a"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sparse_categorical_accuracy=0.019999999552965164,loss=5.130789756774902,keras_training_time_client_sum_sec=0.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEJ-y3YXu7pH"
      },
      "source": [
        "### 3) 100 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7fcwkBu7pH"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train, y_train = ext_classes( x_train_100, y_train_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtpZfkyTu7pH"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9_tnVQKu7pH"
      },
      "source": [
        "x_test = x_test.reshape(-1,3072)\n",
        "x_train = x_train.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FckEqLTs4_-4",
        "outputId": "b24c5477-9b89-472d-8ffc-a1cd0e42f5be"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 100 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n",
            "(500, 3072)\n",
            "(500, 1)\n",
            "<BatchDataset shapes: ((None, 3072), (None, 1)), types: (tf.float64, tf.float64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kk5HIjKu7pI"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loowpzQH5FTp",
        "outputId": "6158564f-2ca2-48fe-d107-dc0ce6a262c2"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:7 out of the last 7 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:8 out of the last 8 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:8 out of the last 8 calls to <function zero_all_if_any_non_finite at 0x7f93cce777a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HVaPn0_u7pI"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDTsGl-0u7pI"
      },
      "source": [
        "NUM_CLIENTS = 100 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // 100 # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(100,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhz1r96Su7pI"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrYtGQLeu7pI"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdcNE9ciu7pJ"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrXvu7Cx5j_7"
      },
      "source": [
        "state, metrics = algorithm.next( state, data )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4FBUXiI5m7W"
      },
      "source": [
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7XnZs4v50CC"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r2HAGXd52_e"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veWUt8aO6u9C"
      },
      "source": [
        "## 1) Every party has instances of every class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u7piAo9NcWH"
      },
      "source": [
        "def ext_classes( data, labels,n, cat_y = True):\n",
        "    data_cl=[]\n",
        "    y=[]\n",
        "    # print(data.shape)\n",
        "    for i in range(n):\n",
        "            # print(data[ np.argwhere( labels == i ).reshape(-1) ].shape)\n",
        "            # print(labels.shape)\n",
        "            # print(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1)].shape,data.shape)\n",
        "            data_cl.append(data[ np.argwhere( labels.reshape(-1) == i ).reshape(-1) ][ : ])\n",
        "            y.extend(np.full((data_cl[i].shape[0]), i, dtype=int))\n",
        "\n",
        "    # print(np.array(data_cl).shape)\n",
        "    x = np.vstack( (data_cl) )\n",
        "    y = np.array(y)\n",
        "    # print(\"In extract classes function\")\n",
        "    # print(x.shape,y.shape)\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI36lI8SO_bt"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train, y_train = ext_classes( x_train_100, y_train_100 ,n=100,cat_y=False)\n",
        "session = tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EwhF66BgL_f"
      },
      "source": [
        "x_test = x_test.reshape(-1,3072)\n",
        "x_train = x_train.reshape(-1,3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS3LZBBPP7cu"
      },
      "source": [
        "x_new = []\n",
        "y_new = []\n",
        "for i in range(500):\n",
        "    for j in range(x_train.shape[0]//500):\n",
        "        x_new.append(x_train[j*500+i])\n",
        "        y_new.append(y_train[j*500+i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ndqWGJbaVqm"
      },
      "source": [
        "x_new = np.array(x_new)\n",
        "x_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm8R_k_ofHsG"
      },
      "source": [
        "y_new = np.array(y_new)\n",
        "y_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeaM8Uuo6u9D"
      },
      "source": [
        "### 1) 10 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEuN1ZnZf5Vy"
      },
      "source": [
        "x_train = x_new.copy()\n",
        "y_train = y_new.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj2C4ogl5chq"
      },
      "source": [
        "\n",
        "# parameters\n",
        "NUM_CLIENTS = 10 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "# shuffle data\n",
        "# idx = np.arange( TOTAL_SAMPLES )\n",
        "# np.random.shuffle( idx )\n",
        "# x_train = x_train[ idx ]\n",
        "# y_train = y_train[ idx ]\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeTXLXa86u9F"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwrAD80z5hEK"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDcKszC6u9G"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bRj9l0z6u9H"
      },
      "source": [
        "NUM_CLIENTS = 10 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(NUM_CLIENT_SAMPLES,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoBH3W1w6u9I"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvwMOPsi6u9I"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shOZzAp_6u9J"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr8D6oJT6u9J"
      },
      "source": [
        "### 2) 20 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_BNSjTP6u9K"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train = x_new.copy()\n",
        "y_train = y_new.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avUiBIA86u9K"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIGYu8QD5q8o"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 20 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "# shuffle data\n",
        "# idx = np.arange( TOTAL_SAMPLES )\n",
        "# np.random.shuffle( idx )\n",
        "# x_train = x_train[ idx ]\n",
        "# y_train = y_train[ idx ]\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1e6dWKc6u9N"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XEvmepD6u9O"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3C8S-3t6u9O"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5b2PAqH6u9P"
      },
      "source": [
        "NUM_CLIENTS = 20 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(NUM_CLIENT_SAMPLES,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvaxHS-_6u9Q"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_esaP0H6u9Q"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-EWbOZ36u9Q"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KdhJ9V6u9R"
      },
      "source": [
        "### 3) 50 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4WSFdpN6u9S"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train = x_new.copy()\n",
        "y_train = y_new.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RASacLdd6u9S"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cff06wF85_aN"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 50 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "# shuffle data\n",
        "# idx = np.arange( TOTAL_SAMPLES )\n",
        "# np.random.shuffle( idx )\n",
        "# x_train = x_train[ idx ]\n",
        "# y_train = y_train[ idx ]\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQi1q2ST6u9W"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1yV0-ic6u9W"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilRqeeKO6u9X"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL9o46fE6u9Y"
      },
      "source": [
        "NUM_CLIENTS = 50 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(NUM_CLIENT_SAMPLES,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uPIh5lZ6u9Y"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zErBKnzz6u9Z"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlpeuOTT6u9Z"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUAo6rqz6u9a"
      },
      "source": [
        "### 4) 100 Clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK4xsLHN6u9b"
      },
      "source": [
        "#extract equal ratio of all the classes for train\n",
        "x_train = x_new.copy()\n",
        "y_train = y_new.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtOILS9R6u9b"
      },
      "source": [
        "#extract equal ratio of all the classes for test\n",
        "x_test, y_test = ext_classes( x_test_100, y_test_100 ,n=100,cat_y=False)\n",
        "session =  tf.compat.v1.Session()\n",
        "tf.compat.v1.keras.backend.set_session( session )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqCcBTQ66eJH"
      },
      "source": [
        "# parameters\n",
        "NUM_CLIENTS = 100 # number of clients\n",
        "TOTAL_SAMPLES = x_train.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 3\n",
        "\n",
        "# shuffle data\n",
        "# idx = np.arange( TOTAL_SAMPLES )\n",
        "# np.random.shuffle( idx )\n",
        "# x_train = x_train[ idx ]\n",
        "# y_train = y_train[ idx ]\n",
        "\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ]\n",
        "  print( x.shape )\n",
        "  y = y_train[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( [-1,1] ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "  ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-irc8j2l6u9d"
      },
      "source": [
        "# define a function that builds our model\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add( tf.keras.layers.Dense( 64, activation='relu', input_shape=( x_train.shape[ 1: ] ) ) )\n",
        "    for  i in range(98):\n",
        "        model.add( tf.keras.layers.Dense( 64, activation='relu' ) )\n",
        "    model.add( tf.keras.layers.Dense( 100, activation='softmax' ) )\n",
        "\n",
        "    return model\n",
        "\n",
        "def model_function():\n",
        "  # we need a dummy batch to build the federated model\n",
        "  # From the docs:\n",
        "  # A nested structure of values that are convertible to batched tensors\n",
        "  # with the same shapes and types as expected by forward_pass(). \n",
        "  # The values of the tensors are not important and can be filled with any \n",
        "  # reasonable input value.\n",
        "  dummy_batch = collections.OrderedDict( [ \n",
        "      ('x', np.ones( ( BATCH_SIZE, x_train.shape[ 1 ] ) ) ),\n",
        "      ('y', np.ones( ( BATCH_SIZE, 1) ) ) ] )\n",
        "\n",
        "  # get the compiled keras model\n",
        "  model = build_model()\n",
        "  # use tensorflow function to create a federated learning model\n",
        "  return tff.learning.from_keras_model( model, loss=tf.keras.losses.SparseCategoricalCrossentropy(),  dummy_batch=dummy_batch, metrics=[tf.keras.metrics.SparseCategoricalAccuracy() ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZMFl1lv6jkS"
      },
      "source": [
        "# use tensorflow to create the averaging algorithm\n",
        "algorithm = tff.learning.build_federated_averaging_process( model_function, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02 ) ) \n",
        "\n",
        "# initialize the learning algorithm and get the initial state\n",
        "state = algorithm.initialize()\n",
        "\n",
        "# run the training steps\n",
        "start = time.time()\n",
        "for e in range( EPOCHS ):\n",
        "    state, metrics = algorithm.next( state, data )\n",
        "    print( 'epoch' , e , metrics )\n",
        "stop = time.time()\n",
        "print(f\"Training time: {stop - start}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05kAABtz6u9e"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTaNxUr6u9e"
      },
      "source": [
        "NUM_CLIENTS = 100 # number of clients\n",
        "TOTAL_SAMPLES = x_test.shape[ 0 ]\n",
        "NUM_CLIENT_SAMPLES = TOTAL_SAMPLES // NUM_CLIENTS # number of samples per client\n",
        "data = []\n",
        "# split into clients\n",
        "for i in range( NUM_CLIENTS ):\n",
        "  x = x_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES  ].reshape(NUM_CLIENT_SAMPLES,1,3072)\n",
        "  print( x.shape )\n",
        "  y = y_test[ i * NUM_CLIENT_SAMPLES : ( i + 1 ) * NUM_CLIENT_SAMPLES ].reshape( -1,1,1 ) \n",
        "  print( y.shape )\n",
        "  ds = tf.data.Dataset.from_tensor_slices( (x.astype( np.float ) , y.astype( np.float ) ) )\n",
        "#   ds = ds.repeat( EPOCHS ).shuffle( 200 ).batch( BATCH_SIZE )\n",
        "\n",
        "  print( ds )\n",
        "  data.append( ds )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn3P3wQP6u9e"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7cVIiXd6u9f"
      },
      "source": [
        "test_metrics = evaluation(state.model, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hwji_F6u9f"
      },
      "source": [
        "print(str(test_metrics))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}